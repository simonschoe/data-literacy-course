# Data Literacy - Ausgewählte Kapitel

#### 1. Einführung „Data Literacy“
Die Veranstaltung startet mit einer ausführlichen Beschreibung und Reflektion des Konzeptes der „Data Literacy“. Dabei wird zunächst der Begriff der „Daten“ geklärt. Anschließend wird die Motivation der Data Literacy untersucht. Sie wird definiert, gegen verwandte Konzepte abgegrenzt und mögliche Inhalte werden anhand des Kompetenzrahmens von Ridsdale et al. (2015) aufgezeigt. Fundierende oder verbundene Themen werden dabei im Zusammenhang erörtert. Letztendlich sollte jeder Teilnehmer den Begriff für sein Verständnis definieren können. Im zweiten Block wird dann der Frage nachgegangen, wo und wie sich über das Internet Informationen und Daten zu ausgewählten Themen finden lassen. Dabei gilt die Aufmerksamkeit sowohl der allgemeinen Recherche wie auch der Identifikation von Datenquellen. Abschließend wird im dritten Block exemplarisch einem besonderen Phänomen in der Datenanalyse nachgegangen.

#### 2. Simulationsrechnung
Einführung in die Simulation. Vorstellung eines Anwendungsfalles (Unternehmensplanspiel zur Lagerhaltung). Im Weiteren wird dann unter Python ein Simulationsmodell dieses Spiels entwickelt, mit welchem die beste Spielpolitik bestimmt werden soll. Die Tabellenkalkulation Excel wird zur Visualisierung der Simulationsergebnisse herangezogen. Die Ergebnisse werden analysiert und interpretiert. Zielführende Entscheidungsempfehlungen werden abgeleitet.

#### 3. Datenvisualisierung
Im Rahmen einer Einführung wird die Gestaltung von Standard-Businessgrafiken (auch praktisch unter Nutzung von Excel) vermittelt. Es wird darüber hinaus auf „exotische“ Grafikformen sowie den Missbrauch grafischer Darstellungen eingegangen. Das Thema Visual Analytics bildet den Übergang zu einer Einführung in die Software „Tableau“, die nicht nur die Visualisierung, sondern auch die Datenanalyse unterstützt. Übungen zu den einzelnen Themen runden den Einblick in die Datenvisualisierung ab.

#### 4. Datenmanipulation & Datenanalyse mit Python
Die Veranstaltung beginnt mit einer Einordnung des Begriffs "Data Science" in den übergeordneten Kontext der "Data Literacy". Es folgt eine Diskussion des Anforderungsprofils des Data Scientists sowie ein Überblick über den generischen Data Science Workflow. Dabei wird aufgezeigt die Kenntnisse einer Programmiersrache (z.B. Python) zur Analyse von Daten heutzutage unabdingbar ist. Der zweite Teil der Veranstaltung beginnt mit einer grundlegenden Einführung in die Programmiersprache Python, gepaart mit diversen praktischen Beispielen und Übungen. Im Fokus steht die Arbeit mit Jupyter Notebooks zur Vertiefung der Inhalte über Datentypen, Datenstrukturen, Vergleichsoperatoren und Funktionen. Im dritten (Haupt-)Teil der Veranstaltung folgt zunächst eine kurze Diskussion verschiedener Datenmodalitäten (Tabellarische Daten, Text, Bild, Audio) bevor die Teilnehmer:innen die grundlegenden Funktionen zur Manipulation und Analyse tabellerischer Daten in Python kennenlernen. Themenschwerpunkte sind dabei der Datenimport, die primären Datenmanipulationen auf Zeilen- und Spalten-Ebene, die Berechnung und Interpretation von Statistiken sowie das Gruppieren und Mergen von Datensätzen. Analog zu Teil zwei wechseln sich dabei theoretische Einheiten mit hands-on Einheiten und Übungen ab. Alle Beispiele werden anhand eines realen Datensatzes und unter Verwendung der Python Library `pandas` verdeutlicht.

#### 5. Verarbeitung von Textdaten
Einführung in die maschinelle Verarbeitung von Textdaten, insb. Datenaufbereitung mittel Regular Expression (Regex) und quantitative Analyse von Textinhalten. Dabei werden beispielhaft einige wesentliche Methoden und Konzepte aus dem Bereich Natural Language Processing (NLP) vorgestellt.

#### 6. Korrelation & Kausalität
Der Themenblock zielt auf die Verinnerlichung von Denkmustern ab, die es erlauben, Korrelationen von kausalen Zusammenhängen zu unterscheiden. Es werden einige Strategien zur Identifikation von kausalen Zusammenhängen besprochen, die es ermöglichen kausale Ursachen-Wirkungs-Beziehungen messbar zu machen.

#### 7. Datenbeschaffung mittels API
Die letzte Vorlesungseinheit gibt einen kurzen Überblick über Programmierschnittstellen (sog. Application Programming Interfaces oder „APIs“), wo diese überall im Internet zu finden sind und wie sie zur Datenbeschaffung genutzt werden können. Es werden einige freu zugängliche Schnittstellen exemplarisch vorgestellt und angewandt (z.B. zum Zwecke der maschinellen Übersetzung, Transkription von Audio-Dateien oder Generierung von Musik-Empfehlungen).

#### 8. Generative KI & Large Language Models
Einführung in die Verwendung und Anwendungsmöglichkeiten generativer KI mit einem Fokus auf Sprachmodelle (Large Language Models, LLMs) wie ChatGPT (OpenAI) oder Gemini (Google). Es wird sowohl das Training dieser Modelle als auch deren Funktionsweise und Anwendungsfälle im betriebswirtschaftlichen Kontext diskutiert. Darüber hinaus behandelt die Vorlesung verschiedene Tipps und Hinweise zur richtigen Verwendung von Sprachmodellen (bzw. Chatbots wie ChatGPT): dem sogenannten „Prompt Engineering“.
