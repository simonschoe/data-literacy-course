# Data Literacy - Ausgewählte Kapitel

### Ziel der Veranstaltung und Vorkenntnisse
Das Ziel dieser Schlüsselqualifikation ist es, Studierenden frühzeitig die wesentlichen Kompetenzen im Umgang und der Arbeit mit Daten („Data Literacy“) zu vermitteln. Insbesondere erlernen die Studierenden dabei strukturiert über die Arbeit mit Daten nachzudenken, Daten zu modellieren und zu interpretieren sowie die entsprechenden praktischen Tools und Technologien entlang des Data Science-Prozesses einzusetzen. Die Inhalte des Kurses bereiten Studierende auf eine potenzielle empirische Abschlussarbeit in der Wissenschaft sowie die Realität der Arbeit mit Daten in der Praxis vor. Theoretische Vortragseinheiten wechseln sich dabei mit anwendungsbezogenen Einheiten zur Vertiefung des Gelernten ab. Studierende sammeln (erste) Erfahrungen im Umgang mit Excel, Python oder Tableau. Es wird erwartet, dass die Studierenden ein intrinsisches Interesse an den behandelten Themen mitbringen und sich aktiv einbringen. Die individuelle Lernkurve hängt dabei von den Vorkenntnissen der Studierenden ab und kann eine konsequente Nachbereitung der Inhalte zwischen der Veranstaltungsterminen erfordern. Generell sollten Teilnehmer:innen folgende Vorkenntnisse mitbringen:
- Erfolgreicher Abschluss der Module „Statistik 1“ und „Statistik 2“, „Mathematik für Wirtschaftswissenschaftlicher“ und „Technikern der IT“.
- Vorkenntnisse im Umgang mit Python oder einer anderen Programmiersprache zur Datenanalyse sind nicht erforderlich. Gleiches gilt für Tableau.


### Veranstaltungsinhalte

##### 1. Einführung „Data Literacy“
Die Veranstaltung startet mit einer ausführlichen Beschreibung und Reflektion des Konzeptes der „Data Literacy“. Dabei wird zunächst der Begriff der „Daten“ geklärt. Anschließend wird die Motivation der Data Literacy untersucht. Sie wird definiert, gegen verwandte Konzepte abgegrenzt und mögliche Inhalte werden anhand des Kompetenzrahmens von Ridsdale et al. (2015) aufgezeigt. Fundierende oder verbun-dene Themen werden dabei im Zusammenhang erörtert. Letztendlich sollte jeder Teilnehmer den Begriff für sein Verständnis definieren können. Im zweiten Block wird dann der Frage nachgegangen, wo und wie sich über das Internet Informationen und Daten zu ausgewählten Themen finden lassen. Dabei gilt die Aufmerksamkeit sowohl der allgemeinen Recherche wie auch der Identifikation von Datenquellen. Abschließend wird im dritten Block exemplarisch einem besonderen Phänomen in der Datenanalyse nachgegangen.

##### 2. Simulationsrechnung
Einführung in die Simulation. Anschließend Vorstellung eines Anwendungsfalles (Unternehmensplanspiel). Im Weiteren wird dann unter Verwendung von Excel und unter Nutzung der integrierten Programmiersprache Visual Basic for Applications (VBA) ein Simulationsmodell entwickelt. Die Ergebnisse werden analysiert und interpretiert. Zielführende Entscheidungsempfehlungen werden abgeleitet.

##### 3. Datenvisualisierung
Im Rahmen einer Einführung wird die Gestaltung von Standard-Businessgrafiken (auch praktisch unter Nutzung von Excel) vermittelt. Es wird darüber hinaus auf „exotische“ Grafikformen sowie den Missbrauch grafischer Darstellungen eingegangen. Das Thema Visual Analytics bildet den Übergang zu einer Einführung in die Software „Tableau“, die nicht nur die Visualisierung, sondern auch die Datenanalyse unterstützt. Übungen zu den einzelnen Themen runden den Einblick in die Datenvisualisierung ab.

##### 4. Datenmanipulation & Datenanalyse mit Python
Zunächst Diskussion des Begriffs „Data Science“ sowie Überblick über den Data Science Prozess. Es folgt eine grundlegende Einführung in die Programmiersprache Python mit diversen Anwendungsbeispielen. Der Großteil des Themenblockes besteht dann aus der praktischen Arbeit mit tabellarischen Daten in Python, insb. Datenmanipulation und Datenanalyse mit der Python Library Pandas.

##### 5. Verarbeitung von Textdaten
Einführung in die maschinelle Verarbeitung von Textdaten, insb. Datenaufbereitung mittel Regular Expression (Regex) und quantitative Analyse von Textinhalten. Dabei werden beispielhaft einige wesentliche Methoden und Konzepte aus dem Bereich Natural Language Processing (NLP) vorgestellt.

##### 6. Korrelation & Kausalität
Der Themenblock zielt auf die Verinnerlichung von Denkmustern ab, die es erlauben, Korrelationen von kausalen Zusammenhängen zu unterscheiden. Es werden einige Strategien zur Identifikation von kausalen Zusammenhängen besprochen, die es ermöglichen kausale Ursachen-Wirkungs-Beziehungen messbar zu machen.

##### 7. Datenbeschaffung mittels API
Die letzte Vorlesungseinheit gibt einen kurzen Überblick über Programmierschnittstellen (sog. Application Programming Interfaces oder „APIs“), wo diese überall im Internet zu finden sind und wie sie zur Datenbeschaffung genutzt werden können. Es werden einige freu zugängliche Schnittstellen exemplarisch vorgestellt und angewandt (z.B. zum Zwecke der maschinellen Übersetzung, Transkription von Audio-Dateien oder Generierung von Musik-Empfehlungen).
